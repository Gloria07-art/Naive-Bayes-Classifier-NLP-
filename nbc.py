# -*- coding: utf-8 -*-
"""nbc.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lHq9xSRLffYqyFcIcqyWG6HHJCBQj7aY

#Lab 2

"""

from google.colab import drive
drive.mount('/content/drive')

"""# 3-Gram"""

import os
import string
import numpy as np
from collections import defaultdict, Counter
from nltk.tokenize import word_tokenize
import re
import nltk
nltk.download('punkt')
import math

def read_books_by_pages(directory, page_length=1000):
    books = {}
    for filename in os.listdir(directory):
        if filename.endswith('.txt'):
            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:
                book_name = filename.split('.')[0]
                full_text = file.read()
                # Split the book into pages
                pages = [full_text[i:i+page_length] for i in range(0, len(full_text), page_length)]
                books[book_name] = pages
    return books

books_directory = "/content/drive/MyDrive/NLP"
harry_potter_books = read_books_by_pages(books_directory)

def tokenize_and_extract_ngrams(text, n):
    # Tokenize
    tokens = word_tokenize(text.lower())

    # Remove punctuation
    tokens = [token for token in tokens if token.isalnum()]

    # Extract N-grams
    ngrams = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]

    return ngrams

def process_pages(books, n):
    processed_books = {}
    for book, pages in books.items():
        processed_books[book] = []
        for page in pages:
            ngrams = tokenize_and_extract_ngrams(page, n)
            processed_books[book].append(Counter(ngrams))
    return processed_books

n = 3
processed_books = process_pages(harry_potter_books, n)

import random
def split_page_data(processed_books, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    train_data, val_data, test_data = {}, {}, {}

    for book, pages in processed_books.items():
        random.shuffle(pages)

        n = len(pages)
        train_end = int(n * train_ratio)
        val_end = train_end + int(n * val_ratio)

        train_data[book] = pages[:train_end]
        val_data[book] = pages[train_end:val_end]
        test_data[book] = pages[val_end:]

    return train_data, val_data, test_data

train_data, val_data, test_data = split_page_data(processed_books)

class NaiveBayesClassifier:
    def __init__(self):
        self.class_probs = {}
        self.feature_probs = {}

    def train(self, train_data):
        total_pages = sum(len(pages) for pages in train_data.values())

        for book, pages in train_data.items():
            self.class_probs[book] = len(pages) / total_pages
            self.feature_probs[book] = {}

            total_words = sum(sum(page.values()) for page in pages)
            vocab = set(word for pages in train_data.values() for page in pages for word in page)

            for word in vocab:
                count = sum(page.get(word, 0) for page in pages)
                self.feature_probs[book][word] = (count + 1) / (total_words + len(vocab))

    def predict(self, features):
        results = {}
        for book in self.class_probs:
            prob = math.log(self.class_probs[book])
            for word, count in features.items():
                if word in self.feature_probs[book]:
                    prob += count * math.log(self.feature_probs[book][word])
            results[book] = prob
        return max(results, key=results.get)

classifier = NaiveBayesClassifier()
classifier.train(train_data)

def evaluate(classifier, data):
    correct = 0
    total = 0
    for book, pages in data.items():
        for page in pages:
            prediction = classifier.predict(page)
            if prediction == book:
                correct += 1
            total += 1
    return correct / total

val_accuracy = evaluate(classifier, val_data)
print(f"Validation Accuracy: {val_accuracy:.2f}")

test_accuracy = evaluate(classifier, test_data)
print(f"Test Accuracy: {test_accuracy:.2f}")

import matplotlib.pyplot as plt

# Function to plot the distribution of 2-grams in each book
def plot_ngram_distribution(processed_books, n=3):
    for book, pages in processed_books.items():
        ngram_counts = Counter()
        for page in pages:
            ngram_counts.update(page)

        most_common_ngrams = ngram_counts.most_common(20)  # Top 20 most common 2-grams
        ngrams, counts = zip(*most_common_ngrams)

        plt.figure(figsize=(10, 6))
        plt.barh(ngrams, counts, color='skyblue')
        plt.xlabel('Frequency')
        plt.title(f'Top 20 {n}-grams in {book}')
        plt.gca().invert_yaxis()  # Invert y-axis to have the highest count on top
        plt.show()

# Assuming processed_books contains 2-grams
plot_ngram_distribution(processed_books, n=3)

# Plotting the performance on validation and test sets
def plot_classifier_performance(classifier, val_data, test_data):
    datasets = {'Validation': val_data, 'Test': test_data}
    accuracies = {}

    for dataset_name, data in datasets.items():
        accuracy = evaluate(classifier, data)
        accuracies[dataset_name] = accuracy

    plt.figure(figsize=(8, 5))
    plt.bar(accuracies.keys(), accuracies.values(), color=['blue', 'green'])
    plt.ylim(0, 1)
    plt.xlabel('Dataset')
    plt.ylabel('Accuracy')
    plt.title('Classifier Performance on Validation and Test Sets')
    plt.show()

# Plotting the classifier performance
plot_classifier_performance(classifier, val_data, test_data)

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Function to plot the confusion matrix
def plot_confusion_matrix(true_labels, predicted_labels):
    cm = confusion_matrix(true_labels, predicted_labels, labels=list(set(true_labels)))
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=list(set(true_labels)), yticklabels=list(set(true_labels)))
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.show()

# Example usage:
# Assuming you have true_labels and predicted_labels from your test or validation data

# Get true labels and predicted labels for the test set
true_labels = []
predicted_labels = []

for true_book, pages in test_data.items():
    for page in pages:
        predicted_book = classifier.predict(page)
        true_labels.append(true_book)
        predicted_labels.append(predicted_book)

# Plot the confusion matrix
plot_confusion_matrix(true_labels, predicted_labels)

"""# Unigram(baseline)"""

def read_books_by_pages(directory, page_length=1000):
    books = {}
    for filename in os.listdir(directory):
        if filename.endswith('.txt'):
            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:
                book_name = filename.split('.')[0]
                full_text = file.read()
                # Split the book into pages
                pages = [full_text[i:i+page_length] for i in range(0, len(full_text), page_length)]
                books[book_name] = pages
    return books

books_directory = "/content/drive/MyDrive/NLP"
harry_potter_books = read_books_by_pages(books_directory)

def tokenize_and_extract_ngrams(text, n):
    # Tokenize
    tokens = word_tokenize(text.lower())

    # Remove punctuation
    tokens = [token for token in tokens if token.isalnum()]

    # Extract N-grams
    ngrams = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]

    return ngrams

def process_pages(books, n):
    processed_books = {}
    for book, pages in books.items():
        processed_books[book] = []
        for page in pages:
            ngrams = tokenize_and_extract_ngrams(page, n)
            processed_books[book].append(Counter(ngrams))
    return processed_books

n = 1
processed_books = process_pages(harry_potter_books, n)

import random
def split_page_data(processed_books, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    train_data, val_data, test_data = {}, {}, {}

    for book, pages in processed_books.items():
        random.shuffle(pages)

        n = len(pages)
        train_end = int(n * train_ratio)
        val_end = train_end + int(n * val_ratio)

        train_data[book] = pages[:train_end]
        val_data[book] = pages[train_end:val_end]
        test_data[book] = pages[val_end:]

    return train_data, val_data, test_data

train_data, val_data, test_data = split_page_data(processed_books)

class NaiveBayesClassifier:
    def __init__(self):
        self.class_probs = {}
        self.feature_probs = {}

    def train(self, train_data):
        total_pages = sum(len(pages) for pages in train_data.values())

        for book, pages in train_data.items():
            self.class_probs[book] = len(pages) / total_pages
            self.feature_probs[book] = {}

            total_words = sum(sum(page.values()) for page in pages)
            vocab = set(word for pages in train_data.values() for page in pages for word in page)

            for word in vocab:
                count = sum(page.get(word, 0) for page in pages)
                self.feature_probs[book][word] = (count + 1) / (total_words + len(vocab))

    def predict(self, features):
        results = {}
        for book in self.class_probs:
            prob = math.log(self.class_probs[book])
            for word, count in features.items():
                if word in self.feature_probs[book]:
                    prob += count * math.log(self.feature_probs[book][word])
            results[book] = prob
        return max(results, key=results.get)

classifier = NaiveBayesClassifier()
classifier.train(train_data)

def evaluate(classifier, data):
    correct = 0
    total = 0
    for book, pages in data.items():
        for page in pages:
            prediction = classifier.predict(page)
            if prediction == book:
                correct += 1
            total += 1
    return correct / total

val_accuracy = evaluate(classifier, val_data)
print(f"Validation Accuracy: {val_accuracy:.2f}")

test_accuracy = evaluate(classifier, test_data)
print(f"Test Accuracy: {test_accuracy:.2f}")
